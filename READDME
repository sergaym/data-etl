# **ğŸ“Š Smart Meter Readings ETL Pipeline**  

## **ğŸ“Œ Overview**  
This repository contains a **technical Data Engineering case study**.  

### **âœ… Project Scope**  
- âœ… **Extracts, transforms, and loads** smart meter readings from **JSON & SQLite**.  
- âœ… **Stores raw data for traceability** and **transforms data for reporting**.  
- âœ… **Implements a modular, scalable architecture**.  
- âœ… **Lays the foundation for automation via Airflow & cloud migration**.  

This version **focuses on batch processing**, with **orchestration, monitoring, and real-time ingestion planned**.

---

## **âš™ï¸ ETL Architecture & Data Flow**  
This ETL pipeline follows a **modular & scalable architecture**, ensuring **maintainability, automation, and easy extensibility**.

```
src/
â”œâ”€â”€ extraction/         # Extracts data from JSON files and SQLite
â”œâ”€â”€ transformation/     # Processes and transforms the data
â”œâ”€â”€ loading/            # Loads raw & transformed data into PostgreSQL
â”œâ”€â”€ pipelines/          # Main ETL orchestration
â””â”€â”€ utils/              # Shared utilities (logging, config)
```

| **Step**            | **Description** |
|---------------------|----------------|
| **Extract**         | Reads raw data from JSON & SQLite. |
| **Store Raw Data**  | Saves extracted data in PostgreSQL (`raw_data` schema). |
| **Transform**       | Processes raw data into structured analytics. |
| **Load Analytics**  | Saves processed data into PostgreSQL (`analytics` schema). |

---

## **ğŸ“Œ Handling `reference_date` in the Pipeline**  

### **ğŸ”¹ Current Implementation**  
- **`reference_date` is set to `2021-01-01` by default**, as per the case study requirements.  
- **Raw data is always ingested fully** â†’ we store all available readings and metadata.  
- **Analytics tables reference `reference_date`** to filter and aggregate data.  

### **ğŸ”¹ Future Improvements for `reference_date` Handling**  
âœ” **Update how we fetch raw data** â†’ Instead of reading everything, **fetch data incrementally** (e.g., from dated folders in S3 or an API with timestamp-based filtering).  
âœ” **Support dynamic `reference_date` changes** â†’ Right now, **it's hardcoded for January 1, 2021**, but in real scenarios, we must **handle different reporting periods** dynamically.  
âœ” **Implement a configuration layer** â†’ Define how `reference_date` should be set for different reports (fixed vs. rolling periods).  
âœ” **Improve Airflow DAG parameterization** â†’ Allow running different `reference_date` values in Airflow dynamically.

---

## **ğŸ“Œ Data Storage Strategy**
This ETL pipeline **preserves both raw and transformed data** to support both **analytics and direct querying** by analysts.

| **Schema**    | **Table**                    | **Purpose** |
|--------------|-----------------------------|-------------|
| **raw_data** | `raw_meter_readings`        | Stores raw JSON readings for full traceability. |
| **raw_data** | `raw_agreements`            | Stores raw contract agreement records. |
| **raw_data** | `raw_products`              | Stores product/tariff details. |
| **raw_data** | `raw_meterpoints`           | Stores raw meterpoint metadata. |
| **analytics** | `active_agreements`         | Agreements valid on **reference_date**. |
| **analytics** | `half_hourly_consumption`   | Aggregated consumption per half-hour. |
| **analytics** | `daily_product_consumption` | Aggregated daily consumption per product. |

âœ… **Why this approach?**  
- **Raw data is stored before transformation** to ensure **data traceability & recovery**.  
- **Transformed data is optimized for analytics & reporting**.

---

## **ğŸ“Œ Known Limitations & Technical Trade-offs**
| **Category** | **Current Limitation** | **Planned Improvements** |
|-------------|-----------------|-------------------------|
| **Scope** | Focuses on **batch ETL**; **event-driven processing not yet implemented**. | Implement **real-time data ingestion** for continuous updates. |
| **Performance** | Currently **single-threaded**, in-memory transformations. | **Migrate to PySpark** for large-scale distributed processing. |
| **Orchestration** | No automated scheduling yet. | Integrate **Airflow/Dagster** for automation & monitoring. |
| **Error Handling** | Basic logging & schema validation. | **Enhance failure recovery** with retries & alerting. |
| **Reference Date Handling** | Hardcoded to `2021-01-01` in analytics. | **Allow configurable reference periods & dynamic execution.** |
| **Cloud Storage** | Reads raw data from local files. | **Migrate raw storage to S3 / cloud-based DB.** |
---

## **ğŸ“Œ Future Roadmap**
| **Feature** | **Why?** | **Priority** |
|------------|---------|-------------|
| **S3 Integration & Cloud Migration** | Move raw data to **S3**, replace SQLite with **PostgreSQL RDS**. | ğŸš¨ Critical |
| **Reference Date Handling Improvements** | Implement **dynamic date selection & automatic period filtering**. | ğŸš€ High |
| **Airflow DAG Optimization** | Implement modular **task-based orchestration**. | ğŸ”¥ High |
| **Monitoring (Grafana/Prometheus)** | Ensure observability & alerts. | ğŸ” Medium |
| **CI/CD (GitHub Actions)** | Automate testing & deployments. | âœ… Medium |
| **Test Coverage & Quality** | Ensure reliability & maintainability. | âš¡ Medium |
| **Schema-Based Data Validation** | Define clear **Pydantic schemas** for structured validation before ingestion. | âœ… Medium |
| **Analytics Modeling (dbt)** | Introduce **dbt** for transformation layer consistency & reusability. | âœ… Medium |
---

## **ğŸ“Œ Usage Instructions**
### **1ï¸âƒ£ Install Dependencies**
```bash
pip install -e .
```
or using Pipenv:
```bash
pipenv install
```

### **2ï¸âƒ£ Run the ETL Manually**
To process data for a specific date:
```bash
python -m src.pipelines.etl.py --reference_date 2021-01-01
```

### **3ï¸âƒ£ Run ETL with Airflow** (ğŸš§ Draft Implementation)
We have drafted an Airflow-based orchestration setup (not yet production-ready):

```bash
# Note: This is a draft implementation
airflow dags trigger meter_readings_etl
```
ğŸ”¹ **Airflow Status:**
- âœ… DAG logic implemented (`meter_readings_etl.py`).
- âœ… Configured for **batch processing** (real-time triggers planned).
- âœ… Integrated with **Docker for local testing**.
- ğŸ”œ **Pending:** Production deployment validation & security enhancements.

---

## **ğŸ“Œ Deployment: Airflow + Docker**
### **ğŸ”¹ Deployment Stack**
| **Component**         | **Technology**        | **Purpose** |
|----------------------|---------------------|-------------|
| **Orchestration**    | **Apache Airflow**  | Manages ETL scheduling. |
| **Containerization** | **Docker**          | Ensures consistent deployment. |
| **Database**         | **PostgreSQL**      | Stores raw & transformed data. |

### **ğŸ”¹ Running Airflow Locally**
```bash
cd deployment
docker-compose up -d
```
To access the Airflow UI, open **`http://localhost:8080`**.

---

## **ğŸ“Œ Bonus: Streamlit App for Analytics** ğŸ‰  
A **Streamlit-based web app** is included for interactive data exploration.

### **âœ… Features**
- View **raw & transformed tables** dynamically.  
- Run **custom SQL queries** on PostgreSQL.  
- Visualize **aggregated consumption trends & agreement activations**.  

### **ğŸ“Œ How to Run**
```bash
streamlit run playground/streamlit-app/main.py
```
Then, open the app in your browser at **`http://localhost:8501`**.

---

## **ğŸ“© Contact & Support**
For any issues or feature requests, please **open a GitHub Issue**.  
For direct queries, contact ğŸ“§ **sergioayala.contacto@gmail.com**.

---

### **ğŸ“Œ TL;DR (Summary)**
âœ… **What it does:** Extracts, transforms & loads smart meter data into PostgreSQL.  
âœ… **How to run:** `python -m src.pipelines.etl --reference_date 2021-01-01`.  
âœ… **Next steps:** Improve **reference date handling, automate with Airflow**, migrate to **S3/cloud**, and enhance **monitoring & testing**.  
